prefetch_factor: 2
pin_memory: True

# ============
# read_group_size:
#   Form subgroups of model comm groups that read data together.
#   Each reader in the group only reads 1/read_group_size of the data
#   which is then all-gathered between the group.
#   This can reduce CPU memory usage as well as increase dataloader throughput.
#   The number of GPUs per model must be divisible by read_group_size.
#   To disable, set to 1.
# ============
read_group_size: ${hardware.num_gpus_per_model}

num_workers:
  training: 1
  validation: 1
  test: 1
batch_size:
  training: 2
  validation: 2
  test: 2

# ============
# Multi-dataset batch composition:
# Each batch will contain a dictionary with samples from all datasets:
# {"dataset_a": tensor_batch_a, "dataset_b": tensor_batch_b, ...}
# All datasets must have the same number of valid samples for synchronization
# ============

# runs only N training batches [N = integer | null]
# if null then we run through all the batches
limit_batches:
  training: null
  validation: null
  test: 20

# Dataset-specific grid indices configurations
# If not specified, all datasets will use the default grid_indices above
grid_indices:
  datasets:
    era5:
      _target_: anemoi.training.data.grid_indices.FullGrid
      nodes_name: ${graph.data}
    mw_t:
      _target_: anemoi.training.data.grid_indices.FullGrid  # Could be different
      nodes_name: ${graph.data}
    surface:
      _target_: anemoi.training.data.grid_indices.FullGrid  # Could be different
      nodes_name: ${graph.data}

# ============
# Multi-Dataset Configuration
# Define multiple datasets that will be synchronized during training
# Each dataset must have the same number of valid time samples
# ============

training:
  datasets:
    era5:
      dataset: ${hardware.paths.data}/${hardware.files.dataset}
      start: 1980
      end: 2020
      frequency: ${data.frequency}
    mw_t:
      dataset: ${hardware.paths.data}/${hardware.files.dataset_b}
      start: 1980
      end: 2020
      frequency: ${data.frequency}
    surface:
      dataset:
      - dataset: ${hardware.paths.data}/${hardware.files.dataset_c}
        start: 1980
        end: 2020
        frequency: ${data.frequency}
        drop: ["z", "lsm", "2d", "sp"]
        rename:
          10u: conv_10u
          10v: conv_10v
          2t: conv_2t
          msl: conv_msl
      - dataset: ${hardware.paths.data}/${hardware.files.dataset_d}
        start: 1980
        end: 2020
        frequency: ${data.frequency}
        select: ["z", "lsm"]
        rename:
          z: conv_z
          lsm: conv_lsm
      start: 1980
      end: 2020
      drop: []

validation_rollout: 1 # number of rollouts to use for validation

# Multi-dataset validation with same datasets, different time period
validation:
  datasets:
    era5:
      dataset: ${hardware.paths.data}/${hardware.files.dataset}
      start: 2021
      end: 2022
      frequency: ${data.frequency}
    mw_t:
      dataset: ${hardware.paths.data}/${hardware.files.dataset_b}
      start: 2021
      end: 2022
      frequency: ${data.frequency}
    surface:
      dataset:
      - dataset: ${hardware.paths.data}/${hardware.files.dataset_c}
        start: 2021
        end: 2022
        frequency: ${data.frequency}
        drop: ["z", "lsm", "2d", "sp"]
        rename:
          10u: conv_10u
          10v: conv_10v
          2t: conv_2t
          msl: conv_msl
      - dataset: ${hardware.paths.data}/${hardware.files.dataset_d}
        start: 2021
        end: 2022
        frequency: ${data.frequency}
        select: ["z", "lsm"]
        rename:
          z: conv_z
          lsm: conv_lsm
      start: 2021
      end: 2022
      drop: []
