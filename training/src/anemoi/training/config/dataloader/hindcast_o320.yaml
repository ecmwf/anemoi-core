prefetch_factor: 2
pin_memory: True

# ============
# read_group_size:
#   Form subgroups of model comm groups that read data together.
#   Each reader in the group only reads 1/read_group_size of the data
#   which is then all-gathered between the group.
#   This can reduce CPU memory usage as well as increase dataloader throughput.
#   The number of GPUs per model must be divisible by read_group_size.
#   To disable, set to 1.
# ============
read_group_size: ${system.hardware.num_gpus_per_model}

num_workers:
  training: 8
  validation: 8
  test: 8
batch_size:
  training: 1
  validation: 1
  test: 1

# ============
# Multi-dataset batch composition:
# Each batch will contain a dictionary with samples from all datasets:
# {"dataset_a": tensor_batch_a, "dataset_b": tensor_batch_b, ...}
# All datasets must have the same number of valid samples for synchronization
# ============

# runs only N training batches [N = integer | null]
# if null then we run through all the batches
limit_batches:
  training: null
  validation: null
  test: 20

# Dataset-specific grid indices configurations
# Each dataset samples from the "data" nodes in its respective graph
# Graph structure: dict with keys "lres", "hres_forcings", "hres"
# Each graph contains: HeteroData(data=..., hidden=...)
# All datasets use nodes_name="data" to sample from the data nodes
grid_indices:
  datasets:
    in_lres:
      _target_: anemoi.training.data.grid_indices.FullGrid
      nodes_name: "data"  # Samples from "data" nodes in "in_lres" graph
    in_hres:
      _target_: anemoi.training.data.grid_indices.FullGrid
      nodes_name: "data"  # Samples from "data" nodes in "in_hres" graph
    out_hres:
      _target_: anemoi.training.data.grid_indices.FullGrid
      nodes_name: "data"  # Samples from "data" nodes in "out_hres" graph


# ============
# Multi-Dataset Configuration
# Define multiple datasets that will be synchronized during training
# Each dataset must have the same number of valid time samples
# ============

steps: [0, 12, 24, 36, 48, 60, 72]
training_date: 2021
valid_date: 2022
test_date: 2023


training:
  start: 1900
  end: 1910
  datasets:
    in_lres:
      dataset: ${system.input.dataset_in_lres}
      start: ${dataloader.training.start}
      end: ${dataloader.training.end}
      frequency: ${data.frequency}
      drop: []
      fake_hindcasts: 
        end:  ${dataloader.training_date}
        steps: ${dataloader.steps}       
    in_hres:
      dataset: ${system.input.dataset_in_hres}
      start: ${dataloader.training.start}
      end: ${dataloader.training.end}
      frequency: ${data.frequency}
      fake_hindcasts: 
        end:  ${dataloader.training_date}
        steps: ${dataloader.steps}       
      drop: []
    out_hres:
      dataset: ${system.input.dataset_out_hres}
      start: ${dataloader.training.start}
      end: ${dataloader.training.end}
      frequency: ${data.frequency}
      drop: []    
      fake_hindcasts: 
        end:  ${dataloader.training_date}
        steps: ${dataloader.steps}       

validation_rollout: 0 # number of rollouts to use for validation

# Multi-dataset validation with same datasets, different time period
validation:
  start: 1900
  end: 1910
  datasets:
    in_lres:
      dataset: ${system.input.dataset_in_lres}
      start: ${dataloader.training.start}
      end: ${dataloader.training.end}
      frequency: ${data.frequency}
      drop: []
      fake_hindcasts: 
        start: ${dataloader.valid_date}
        end:  ${dataloader.valid_date}
        steps: ${dataloader.steps}       
    in_hres:
      dataset: ${system.input.dataset_in_hres}
      start: ${dataloader.training.start}
      end: ${dataloader.training.end}
      frequency: ${data.frequency}
      fake_hindcasts: 
        start: ${dataloader.valid_date}
        end:  ${dataloader.valid_date}
        steps: ${dataloader.steps}       
      drop: []
    out_hres:
      dataset: ${system.input.dataset_out_hres}
      start: ${dataloader.training.start}
      end: ${dataloader.training.end}
      frequency: ${data.frequency}
      drop: []    
      fake_hindcasts: 
        start: ${dataloader.valid_date}
        end:  ${dataloader.valid_date}
        steps: ${dataloader.steps}     

# Multi-dataset test with same datasets, different time period
test:
  start: 1900
  end: 1910
  datasets:
    in_lres:
      dataset: ${system.input.dataset_in_lres}
      start: ${dataloader.training.start}
      end: ${dataloader.training.end}
      frequency: ${data.frequency}
      drop: []
      fake_hindcasts: 
        start: ${dataloader.test_date}
        end:  ${dataloader.test_date}
        steps: ${dataloader.steps}       
    in_hres:
      dataset: ${system.input.dataset_in_hres}
      start: ${dataloader.training.start}
      end: ${dataloader.training.end}
      frequency: ${data.frequency}
      fake_hindcasts: 
        start: ${dataloader.test_date}
        end:  ${dataloader.test_date}
        steps: ${dataloader.steps}       
      drop: []
    out_hres:
      dataset: ${system.input.dataset_out_hres}
      start: ${dataloader.training.start}
      end: ${dataloader.training.end}
      frequency: ${data.frequency}
      fake_hindcasts: 
        start: ${dataloader.test_date}
        end:  ${dataloader.test_date}
        steps: ${dataloader.steps}     
      drop: []

