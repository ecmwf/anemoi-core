defaults:
  - hardware: slurm
  - data: zarr
  - dataloader: native_grid
  - model: transformer
  - graph: encoder_decoder_only
  - training: default
  - diagnostics: evaluation
  - override hydra/job_logging: none
  - override hydra/hydra_logging: none
  - _self_
hydra:
  job:
    chdir: True
  output_subdir: null #disable outputs/date/date/.hydra from being made in cwd

no_validation: True

model: #transformer only
  num_channels: 1024
  processor:
    attention_implementation: 'flash_attention_v3'
    window_size: 1120
  layer_kernels:
    LayerNorm:
      _target_: "torch.nn.LayerNorm" #the default PyTorch implementation
            #_target_: "liger_kernel.transformers.LigerRMSNorm"
      _partial_: True
      #Any arguments to your chosen function go here e.g.
      #bias: False
    Linear:
      _target_: "torch.nn.Linear"
      _partial_: True

graph:
  nodes: #hidden res o96 for the transformer model
    hidden:
      node_builder:
        grid: o96
        #npz_file: ${oc.decode:${oc.env:SCRATCH}}/aifs/inputs/grids/grid-o96.npz

dataloader:
  batch_size:
    training: 1
    validation: 1
  training:
    end: 20230125
  validation:
    start: 20230126
    end: 20230131

hardware:
  files:
    dataset: aifs-od-an-oper-0001-mars-o1280-2023-2023-6h-v1-one-month-8gridchunks.zarr
    graph: hackathon.graph
  paths:
    data: ${oc.decode:${oc.env:SCRATCH}}/aifs/inputs
    output: ${oc.decode:${oc.env:SCRATCH}}/aifs/outputs
  num_gpus_per_node: ${oc.decode:${oc.env:SLURM_NTASKS_PER_NODE}}
  num_nodes: ${oc.decode:${oc.env:SLURM_NNODES}}
  num_gpus_per_model: ${oc.decode:${oc.env:SLURM_N}}

diagnostics:
  enable_checkpointing: True
  benchmark_profiler:
    memory:
      enabled: true
      trace_rank0_only: true
    snapshot:
      enabled: false
    time:
      verbose: true
  log:
    wandb: #log offline every 50 steps
      enabled: False
      entity: ''
    mlflow: #log offline every 50 steps
      enabled: False
      offline: True
      tracking_uri: ''
    interval: 50
  plot: #disable plotting
    callbacks: []

training: #disable sanity checking at the start of training
  num_sanity_val_steps: 0