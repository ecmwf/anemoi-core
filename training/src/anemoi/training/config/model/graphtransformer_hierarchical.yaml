num_channels: 1024
cpu_offload: False

keep_batch_sharded: True

model:
  _target_: anemoi.models.models.AnemoiModelEncProcDecHierarchical

presets:
  providers:
    data_to_hidden1:
      _target_: anemoi.models.layers.graph_providers.StaticGraphProvider
      edges: [data, to, hidden_1]
      edge_attributes: ${model.attributes.edges}
      trainable_size: ${model.trainable_parameters.data2hidden}
    hidden1_to_data:
      _target_: anemoi.models.layers.graph_providers.StaticGraphProvider
      edges: [hidden_1, to, data]
      edge_attributes: ${model.attributes.edges}
      trainable_size: ${model.trainable_parameters.hidden2data}
    hidden1_self:
      _target_: anemoi.models.layers.graph_providers.StaticGraphProvider
      edges: [hidden_1, to, hidden_1]
      edge_attributes: ${model.attributes.edges}
      trainable_size: ${model.trainable_parameters.hidden2hidden}
    hidden2_self:
      _target_: anemoi.models.layers.graph_providers.StaticGraphProvider
      edges: [hidden_2, to, hidden_2]
      edge_attributes: ${model.attributes.edges}
      trainable_size: ${model.trainable_parameters.hidden2hidden}
    hidden3_self:
      _target_: anemoi.models.layers.graph_providers.StaticGraphProvider
      edges: [hidden_3, to, hidden_3]
      edge_attributes: ${model.attributes.edges}
      trainable_size: ${model.trainable_parameters.hidden2hidden}
    down_1_2:
      _target_: anemoi.models.layers.graph_providers.StaticGraphProvider
      edges: [hidden_1, to, hidden_2]
      edge_attributes: ${model.attributes.edges}
      trainable_size: ${model.trainable_parameters.data2hidden}
    down_2_3:
      _target_: anemoi.models.layers.graph_providers.StaticGraphProvider
      edges: [hidden_2, to, hidden_3]
      edge_attributes: ${model.attributes.edges}
      trainable_size: ${model.trainable_parameters.data2hidden}
    up_3_2:
      _target_: anemoi.models.layers.graph_providers.StaticGraphProvider
      edges: [hidden_3, to, hidden_2]
      edge_attributes: ${model.attributes.edges}
      trainable_size: ${model.trainable_parameters.hidden2data}
    up_2_1:
      _target_: anemoi.models.layers.graph_providers.StaticGraphProvider
      edges: [hidden_2, to, hidden_1]
      edge_attributes: ${model.attributes.edges}
      trainable_size: ${model.trainable_parameters.hidden2data}

# Edges must match graph/hierarchical_3level.yaml exactly.
graph_providers:
  encoder: ${model.presets.providers.data_to_hidden1}
  decoder: ${model.presets.providers.hidden1_to_data}
  processor: ${model.presets.providers.hidden3_self}
  downscale:
    hidden_1: ${model.presets.providers.down_1_2}
    hidden_2: ${model.presets.providers.down_2_3}
  upscale:
    hidden_2: ${model.presets.providers.up_2_1}
    hidden_3: ${model.presets.providers.up_3_2}
  down_level_processor:
    hidden_1: ${model.presets.providers.hidden1_self}
    hidden_2: ${model.presets.providers.hidden2_self}
  up_level_processor:
    hidden_1: ${model.presets.providers.hidden1_self}
    hidden_2: ${model.presets.providers.hidden2_self}

layer_kernels:
  # The layer_kernels can be adjusted per model component, but are defined here for convenience.
  LayerNorm:
    _target_: torch.nn.LayerNorm
  Linear:
    _target_: torch.nn.Linear
  Activation:
    _target_: torch.nn.GELU
  QueryNorm:
    _target_: anemoi.models.layers.normalization.AutocastLayerNorm
    bias: False
  KeyNorm:
    _target_: anemoi.models.layers.normalization.AutocastLayerNorm
    bias: False

processor:
  _target_: anemoi.models.layers.processor.GraphTransformerProcessor
  trainable_size: ${model.graph_providers.processor.trainable_size}
  sub_graph_edge_attributes: ${model.graph_providers.processor.edge_attributes}
  num_layers: 16
  num_chunks: 4
  mlp_hidden_ratio: 4 # GraphTransformer or Transformer only
  num_heads: 16 # GraphTransformer or Transformer only
  qk_norm: False
  cpu_offload: ${model.cpu_offload}
  layer_kernels: ${model.layer_kernels}
  graph_attention_backend: "triton"  # Options: "triton", "pyg"
  edge_pre_mlp: False

encoder:
  _target_: anemoi.models.layers.mapper.GraphTransformerForwardMapper
  trainable_size: ${model.graph_providers.encoder.trainable_size}
  sub_graph_edge_attributes: ${model.graph_providers.encoder.edge_attributes}
  num_chunks: 4
  mlp_hidden_ratio: 4 # GraphTransformer or Transformer only
  num_heads: 16 # GraphTransformer or Transformer only
  qk_norm: False
  cpu_offload: ${model.cpu_offload}
  layer_kernels: ${model.layer_kernels}
  shard_strategy: "edges"
  graph_attention_backend: "triton"  # Options: "triton", "pyg"
  edge_pre_mlp: False

decoder:
  _target_: anemoi.models.layers.mapper.GraphTransformerBackwardMapper
  trainable_size: ${model.graph_providers.decoder.trainable_size}
  sub_graph_edge_attributes: ${model.graph_providers.decoder.edge_attributes}
  num_chunks: 4
  mlp_hidden_ratio: 4 # GraphTransformer or Transformer only
  num_heads: 16 # GraphTransformer or Transformer only
  initialise_data_extractor_zero: False
  qk_norm: False
  cpu_offload: ${model.cpu_offload}
  layer_kernels: ${model.layer_kernels}
  shard_strategy: "edges"
  graph_attention_backend: "triton"  # Options: "triton", "pyg"
  edge_pre_mlp: False

residual: ${truncation.residual}

output_mask:
  _target_: anemoi.training.utils.masks.NoOutputMask

trainable_parameters:
  data: 8
  hidden: 8
  data2hidden: 8
  hidden2data: 8
  hidden2hidden: 8 # GNN and GraphTransformer Processor only

# Torch compile configuration
# A list of modules present in the model, which will be compiled
# You can optionally pass options to torch.compile with the 'options' key
#
# Below is an explanation of some common parameters to torch.compile
# For a full list of possible parameters, consult the documenation for torch compile
#       https://docs.pytorch.org/docs/stable/generated/torch.compile.html
#
# dynamic (bool): When True, it will try to avoid recompilation by generating
#       as general a kernel as possible. But the performance of the general
#       kernel might be worse. When False, it will generate a specific
#       kernel for each input shape (until the configurable recompile
#       limit has been hit), leading to possibly better performance but
#       more recompilations
# mode (str): Different compilation modes, allowing you to trade off
#       compilation time versus potential performance. See the
#       torch.compile documentation for list of possible modes
# fullgraph (bool): When True, torch.compile will error when it hits a
#       section of code it can't compile. When False, it will fallback to
#       non-compiled ("eager") execution for those lines.
# options (dict): a dict of further options which can be passed to torch.compile
compile:
  - module: anemoi.models.layers.conv.GraphTransformerConv

attributes:
  edges:
    - edge_length
    - edge_dirs
  nodes: []

# Bounding configuration
bounding: #These are applied in order
  # Bound tp (total precipitation) with a Relu bounding layer
  # ensuring a range of [0, infinity) to avoid negative precipitation values.
  - _target_: anemoi.models.layers.bounding.ReluBounding #[0, infinity)
    variables:
      - tp

  # [OPTIONAL] Bound cp (convective precipitation) as a fraction of tp.
  # This guarantees that cp is physically consistent with tp by restricting cp
  # to a fraction of tp [0 to 1]. Uncomment the lines below to apply.
  # NOTE: If this bounding strategy is used, the normalization of cp must be
  # changed to "std" normalization, and the "cp" statistics should be remapped
  # to those of tp to ensure consistency.

  # - _target_: anemoi.models.layers.bounding.FractionBounding # fraction of tp
  #   variables:
  #   - cp
  #   min_val: 0
  #   max_val: 1
  #   total_var: tp

  # [OPTIONAL] NormalizedReluBounding
  # This is an extension of the Relu bounding in case the thrshold to be used
  # is not 0. For example, in case of the sea surface temperature we don't use
  # [0, infinity), buth rather [-2C, infinity). We do not want the water
  # temperature to be below the freezing temperature.

  # - _target_: anemoi.models.layers.bounding.NormalizedReluBounding
  #   variables: [sst]
  #   min_val: [-2]
  #   normalizer: ['mean-std']
