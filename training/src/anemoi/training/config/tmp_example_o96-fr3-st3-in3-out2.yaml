######################################################################
# This is a (temporary) example configuration file illustrating how to
# set up a training run for a model with two output steps (e.g. 3h and 6h ahead)
#########################################################################

config_validation: true

#####################################################################
#DATA
#####################################################################

data:
  format: zarr # default for anemoi-datasets
  # Time frequency requested from dataset
  frequency: 3h
  # Time step of model (must be multiple of frequency)
  timestep: 3h
  forcing:
  - "cos_latitude"
  - "cos_longitude"
  - "sin_latitude"
  - "sin_longitude"
  - "cos_julian_day"
  - "cos_local_time"
  - "sin_julian_day"
  - "sin_local_time"
  - "insolation"
  - "lsm"
  - "z"
  diagnostic:
  - tp
  remapped:
  normalizer:
    default: "mean-std"
    std:
    - "tp"
    min-max:
    max:
    - "z"
    none:
    - "cos_latitude"
    - "cos_longitude"
    - "sin_latitude"
    - "sin_longitude"
    - "cos_julian_day"
    - "cos_local_time"
    - "sin_julian_day"
    - "sin_local_time"
    - "insolation"
    - "lsm"

  imputer:
    default: "none"
  remapper:
    default: "none"
  processors:
    normalizer:
      _target_: anemoi.models.preprocessing.normalizer.InputNormalizer
      config: ${data.normalizer}
  num_features: null # number of features in the forecast state

#####################################################################
#DATAMODULE
#####################################################################

datamodule:
  _target_: anemoi.training.data.datamodule.AnemoiDatasetsDataModule

#####################################################################
#DATALOADER
#####################################################################
dataloader:
  prefetch_factor: 2
  pin_memory: True
  read_group_size: ${hardware.num_gpus_per_model}
  num_workers:
    training: 3
    validation: 3
    test: 3
  batch_size:
    training: 2
    validation: 2
    test: 1

  limit_batches:
    training: null
    validation: null
    test: 1

  dataset:
    dataset: ${hardware.paths.data}/${hardware.files.dataset}
    select:
    - 10u
    - 10v
    - 2d
    - 2t
    - msl
    - sp
    - skt
    - z
    - tp
    - q_50
    - q_100
    - q_150
    - q_200
    - q_250
    - q_300
    - q_400
    - q_500
    - q_700
    - q_850
    - q_925
    - q_1000
    - t_50
    - t_100
    - t_150
    - t_200
    - t_250
    - t_300
    - t_400
    - t_500
    - t_700
    - t_850
    - t_925
    - t_1000
    - u_50
    - u_100
    - u_150
    - u_200
    - u_250
    - u_300
    - u_400
    - u_500
    - u_700
    - u_850
    - u_925
    - u_1000
    - v_50
    - v_100
    - v_150
    - v_200
    - v_250
    - v_300
    - v_400
    - v_500
    - v_700
    - v_850
    - v_925
    - v_1000
    - z_50
    - z_100
    - z_150
    - z_200
    - z_250
    - z_300
    - z_400
    - z_500
    - z_700
    - z_850
    - z_925
    - z_1000
    - lsm
    - insolation
    - cos_julian_day
    - cos_latitude
    - cos_local_time
    - cos_longitude
    - sin_julian_day
    - sin_latitude
    - sin_local_time
    - sin_longitude
  training:
    dataset: ${dataloader.dataset}
    start: 1980-01-01T00:00:00
    end: 2019-12-31T21:00:00
    frequency: ${data.frequency}
    drop: []
  validation_rollout: 24 #number of steps to rollout during validation
  validation:
    dataset: ${dataloader.dataset}
    start: 2020-01-01T00:00:00
    end: 2021-12-31T21:00:00
    frequency: ${data.frequency}
    drop: []
  test:
    dataset: ${dataloader.dataset}
    start: 2022-01-01T00:00:00
    end: null
    frequency: ${data.frequency}

  grid_indices:
    _target_: anemoi.training.data.grid_indices.FullGrid
    nodes_name: ${graph.data}

#####################################################################
#GRAPH
#####################################################################
graph:
  overwrite: False
  data: data
  hidden: hidden

#####################################################################
#MODEL
#####################################################################
model:
  num_channels: 1024
  cpu_offload: False

  keep_batch_sharded: True
  output_mask:
    _target_: anemoi.training.utils.masks.NoOutputMask

  model:
    _target_: anemoi.models.models.AnemoiModelEncProcDec

  layer_kernels:
    LayerNorm:
      _target_: torch.nn.LayerNorm
      #_partial_: True
    Linear:
      _target_: torch.nn.Linear
      #_partial_: True
    Activation:
      _target_: torch.nn.GELU
    QueryNorm:
      _target_: anemoi.models.layers.normalization.AutocastLayerNorm
      bias: False
    KeyNorm:
      _target_: anemoi.models.layers.normalization.AutocastLayerNorm
      bias: False

  processor:
    _target_: anemoi.models.layers.processor.GraphTransformerProcessor
    trainable_size: ${model.trainable_parameters.hidden2hidden}
    sub_graph_edge_attributes: ${model.attributes.edges}
    num_layers: 16
    num_chunks: 4
    mlp_hidden_ratio: 4 # GraphTransformer or Transformer only
    num_heads: 16 # GraphTransformer or Transformer only
    qk_norm: False
    cpu_offload: ${model.cpu_offload}
    layer_kernels: ${model.layer_kernels}
  encoder:
    _target_: anemoi.models.layers.mapper.GraphTransformerForwardMapper
    trainable_size: ${model.trainable_parameters.data2hidden}
    sub_graph_edge_attributes: ${model.attributes.edges}
    shard_strategy: edges
    num_chunks: 2
    mlp_hidden_ratio: 4 # GraphTransformer or Transformer only
    num_heads: 16 # GraphTransformer or Transformer only
    qk_norm: False
    cpu_offload: ${model.cpu_offload}
    layer_kernels: ${model.layer_kernels}
  decoder:
    _target_: anemoi.models.layers.mapper.GraphTransformerBackwardMapper
    trainable_size: ${model.trainable_parameters.hidden2data}
    sub_graph_edge_attributes: ${model.attributes.edges}
    shard_strategy: edges
    num_chunks: 2
    mlp_hidden_ratio: 4 # GraphTransformer or Transformer only
    num_heads: 16 # GraphTransformer or Transformer only
    initialise_data_extractor_zero: False
    qk_norm: False
    cpu_offload: ${model.cpu_offload}
    layer_kernels: ${model.layer_kernels}
  trainable_parameters:
    data: 8
    hidden: 8
    data2hidden: 8
    hidden2data: 8
    hidden2hidden: 8


  attributes:
    edges:
    - edge_length
    - edge_dirs
    nodes: []

  # Bounding configuration
  bounding: #These are applied in order
    # Bound tp (total precipitation) with a Relu bounding layer
    # ensuring a range of [0, infinity) to avoid negative precipitation values.
    - _target_: anemoi.models.layers.bounding.ReluBounding #[0, infinity)
      variables:
      - tp

#####################################################################
#TRAINING
#####################################################################
training:
  run_id: null
  fork_run_id: null
  load_weights_only: False
  transfer_learning: False

 # select model
  model_task: anemoi.training.train.tasks.GraphForecaster
  deterministic: False

  precision: 16-mixed

  # multistep input
  # 1 = single step scheme, X(t-1) used to predict X(t)
  # k > 1: multistep scheme, uses [X(t-k), X(t-k+1), ... X(t-1)] to predict X(t)
  # Deepmind use k = 2 in their model
  multistep_input: 3
  multistep_output: 2

  # gradient accumulation across K batches, K >= 1 (if K == 1 then no accumulation)
  # the effective batch size becomes num-devices * batch_size * k
  accum_grad_batches: 1

  num_sanity_val_steps: 6

  # clipp gradients, 0 : don't clip, default algorithm: norm, alternative: value
  gradient_clip:
    val: 32.
    algorithm: value

  # stochastic weight averaging
  # https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/
  swa:
    enabled: False
    lr: 1.e-4

  # Optimizer settings
  optimizer:
    zero: False # use ZeroRedundancyOptimizer ; saves memory for larger models
    kwargs:
      betas: [0.9, 0.95]

  # select strategy
  strategy:
    _target_: anemoi.training.distributed.strategy.DDPGroupStrategy
    num_gpus_per_model: ${hardware.num_gpus_per_model}
    read_group_size: ${dataloader.read_group_size}

  # loss functions

  # dynamic rescaling of the loss gradient
  # see https://arxiv.org/pdf/2306.06079.pdf, section 4.3.2
  # don't enable this by default until it's been tested and proven beneficial

  loss_gradient_scaling: False


  # length of the "rollout" window (see Keisler's paper)
  rollout:
    start: 1
    # increase rollout every n epochs
    epoch_increment: 0
    # maximum rollout to use
    max: 1

  max_epochs: null # set max_epochs or max_steps. Training stops at the first limit reached.
  max_steps: 260000 #NOTE: increase to 300,000 for high-accuracy full-scale run on LUMI

  lr:
    warmup: 1000
    rate: 3.125e-05
    iterations: ${training.max_steps}
    min: 3e-7

  scalers:
    # Several scalers can be added here. In order to be applied their names must be included in the loss.
    # scaler name must be included in `scalers` in the losses for this to be applied.
    general_variable:
      #Â Variable groups definition for scaling by variable level.
      # The variable level scaling methods are defined under additional_scalers
      # A default group is required and is appended as prefix to the metric of all variables not assigned to a group.
      _target_: anemoi.training.losses.scalers.GeneralVariableLossScaler
      weights:
        default: 1
        q: 0.6 #1
        t: 6   #1
        u: 0.8 #0.5
        v: 0.5 #0.33
        w: 0.001
        z: 12  #1
        sp: 10
        10u: 0.1
        10v: 0.1
        2d: 0.5
        tp: 0.025
        cp: 0.0025

    pressure_level:
      _target_: anemoi.training.losses.scalers.ReluVariableLevelScaler
      group: pl
      y_intercept: 0.2
      slope: 0.001

    # mask NaNs with zeros in the loss function
    nan_mask_weights:
      _target_: anemoi.training.losses.scalers.NaNMaskScaler

    # tendency scalers
    # scale the prognostic losses by the stdev of the variable tendencies (e.g. the 6-hourly differences of the data)
    # useful if including slow vs fast evolving variables in the training (e.g. Land/Ocean vs Atmosphere)
    # if using this option 'variable_loss_scalings' should all be set close to 1.0 for prognostic variables
    # stdev_tendency:
    #   _target_: anemoi.training.losses.scalers.StdevTendencyScaler

    # var_tendency:
    #   _target_: anemoi.training.losses.scalers.VarTendencyScaler

    # Scalers from node attributes
    node_weights:
      _target_: anemoi.training.losses.scalers.GraphNodeAttributeScaler
      nodes_name: ${graph.data}
      nodes_attribute_name: area_weight
      norm: "unit-sum"

    output_steps:
      _target_: anemoi.training.losses.scalers.OutputStepScaler
      norm: "unit-sum"
      weights: #ordered chronologically going downwards
        - 1.0
        - 10.0

    first_output_step:
      _target_: anemoi.training.losses.scalers.OutputStepScaler
      norm: "unit-sum"
      weights: #ordered chronologically going downwards
        - 1.0
        - 0.0

    last_output_step:
      _target_: anemoi.training.losses.scalers.OutputStepScaler
      norm: "unit-sum"
      weights: #ordered chronologically going downwards
        - 0.0
        - 1.0

  # loss function for the model
  training_loss:
    # loss class to initialise
    _target_: anemoi.training.losses.MSELoss
    #Â Scalers to include in loss calculation
    # A selection of available scalers are listed in training/scalers/scalers.yaml
    # '*' is a valid entry to use all `scalers` given, if a scaler is to be excluded
    # add `!scaler_name`, i.e. ['*', '!scaler_1'], and `scaler_1` will not be added.
    scalers: ['pressure_level', 'general_variable', 'nan_mask_weights', 'node_weights', 'output_steps']
    ignore_nans: False

  # Validation metrics calculation,
  #Â This may be a list, in which case all metrics will be calculated
  # and logged according to their name.
  # These metrics are calculated in the output model space, and thus
  # have undergone postprocessing.
  validation_metrics:
  # loss class to initialise
    mse_step0:
      _target_: anemoi.training.losses.MSELoss
      #Â Scalers to include in loss calculation
      # Cannot scale over the variable dimension due to possible remappings.
      # Available scalers include:
      # - 'loss_weights_mask': Giving imputed NaNs a zero weight in the loss function
      # Use the `scale_validation_metrics` section to variable scale.
      scalers: ['node_weights', 'first_output_step']
      # other kwargs
      ignore_nans: True

    mse_step1:
      _target_: anemoi.training.losses.MSELoss
      #Â Scalers to include in loss calculation
      # Cannot scale over the variable dimension due to possible remappings.
      # Available scalers include:
      # - 'loss_weights_mask': Giving imputed NaNs a zero weight in the loss function
      # Use the `scale_validation_metrics` section to variable scale.
      scalers: ['node_weights', 'last_output_step']
      # other kwargs
      ignore_nans: True

    mse:
      _target_: anemoi.training.losses.MSELoss
      #Â Scalers to include in loss calculation
      # Cannot scale over the variable dimension due to possible remappings.
      # Available scalers include:
      # - 'loss_weights_mask': Giving imputed NaNs a zero weight in the loss function
      # Use the `scale_validation_metrics` section to variable scale.
      scalers: ['node_weights', 'output_steps']
      # other kwargs
      ignore_nans: True

  variable_groups:
    default: sfc
    pl: [q, t, u, v, w, z]

  metrics:
  - z_500
  - t_850
  - u_850
  - v_850


  submodules_to_freeze: []

#####################################################################
#HARDWARE
#####################################################################
hardware:
  accelerator: auto
  num_gpus_per_node: ${oc.decode:${oc.env:SLURM_GPUS_PER_NODE}}
  num_nodes: ${oc.decode:${oc.env:SLURM_NNODES}}
  num_gpus_per_model: 1
  paths:
    data: ${oc.env:DATASETS}/
    output: ${oc.env:RECORDS_DIR}/
    logs:
      base: ${hardware.paths.output}/
      wandb: ${hardware.paths.logs.base}
      mlflow: ${hardware.paths.logs.base}mlflow/
      tensorboard: ${hardware.paths.logs.base}tensorboard/
    checkpoints: ${oc.env:PROD_CKPT}/
    plots: ${hardware.paths.output}plots/
    profiler: ${hardware.paths.output}profiler/
    graph: XXX/global-era-o96
  files:
    dataset: aifs-ea-an-oper-0001-mars-o96-1979-2022-1h-v4.zarr
    graph: global-era-o96.pt
    checkpoint:
      every_n_epochs: anemoi-by_epoch-epoch_{epoch:03d}-step_{step:06d}
      every_n_train_steps: anemoi-by_step-epoch_{epoch:03d}-step_{step:06d}
      every_n_minutes: anemoi-by_time-epoch_{epoch:03d}-step_{step:06d}
    warm_start: null

#####################################################################
#DIAGNOSTICS
#####################################################################
diagnostics:
  benchmark_profiler:
    # Use anemoi-profile to profile the training process
    memory:
      enabled: False
      steps: 5 # wait warmup steps and then do steps (too many steps would lead to a big file)
      warmup: 2
      extra_plots: False
      trace_rank0_only: False #set to true and it will profile rank 0 only. Reads SLURM_PROC_ID so won't work when not running via Slurm
    time:
      enabled: True
      verbose: False #If true, output every action the profiler caputres, otherwise output a subset defined in PROFILER_ACTIONS at the top of aifs/diagnostics/profiler.py
    speed:
      enabled: True
    system:
      enabled: False
    model_summary:
      enabled: False
    snapshot:
      enabled: False
      steps: 4 # wait warmup steps and then do steps
      warmup: 0
  
  callbacks:
  # Add callbacks here
  - _target_: anemoi.training.diagnostics.callbacks.evaluation.RolloutEval
    rollout: ${dataloader.validation_rollout}
    every_n_batches: 50
  

  plot:
    asynchronous: True # Whether to plot asynchronously
    datashader: True # Choose which technique to use for plotting
    frequency: # Frequency of the plotting
      batch: 750
      epoch: 5

    parameters:
    - z_500
    - t_850
    - u_850
    - v_850
    - 2t
    - 10u
    - 10v
    - sp
    - tp
    - msl
    sample_idx: 0
    precip_and_related_fields: [tp] #, cp]
    # select special colormap for precip fields
    colormaps:
      precip:
        _target_: anemoi.training.utils.custom_colormaps.MatplotlibColormapClevels
        clevels: ["#ffffff", "#04e9e7", "#019ff4", "#0300f4", "#02fd02", "#01c501", "#008e00", "#fdf802", "#e5bc00", "#fd9500", "#fd0000", "#d40000", "#bc0000", "#f800fd"]
        variables: ${diagnostics.plot.precip_and_related_fields}
    callbacks: []
      ##Add plot callbacks here
      # - _target_: anemoi.training.diagnostics.callbacks.plot.PlotLoss
      #   # group parameters by categories when visualizing contributions to the loss
      #   # one-parameter groups are possible to highlight individual parameters
      #   parameter_groups:
      #     moisture: [tp, tcw]
      #     sfc_wind: [10u, 10v]
      # - _target_: anemoi.training.diagnostics.callbacks.plot.PlotSample
      #   sample_idx: ${diagnostics.plot.sample_idx}
      #   per_sample : 6
      #   parameters: ${diagnostics.plot.parameters}
      #   #Defining the accumulation levels for precipitation related fields and the colormap
      #   accumulation_levels_plot: [0, 0.05, 0.1, 0.25, 0.5, 1, 1.5, 2, 3, 4, 5, 6, 7, 100] # in mm
      #   cmap_accumulation: ["#ffffff", "#04e9e7", "#019ff4", "#0300f4", "#02fd02", "#01c501", "#008e00", "#fdf802", "#e5bc00", "#fd9500", "#fd0000", "#d40000", "#bc0000", "#f800fd"]
      #   precip_and_related_fields: ${diagnostics.plot.precip_and_related_fields}
      # - _target_: anemoi.training.diagnostics.callbacks.plot.PlotHistogram
      #   sample_idx: ${diagnostics.plot.sample_idx}
      #   precip_and_related_fields: ${diagnostics.plot.precip_and_related_fields}
      #   parameters:
      #   - z_500
      #   - tp
      #   - 2t
      #   - 10u
      #   - 10v
      # - _target_:  anemoi.training.diagnostics.callbacks.plot.LongRolloutPlots
      #   # for rollout and video_rollout pick any integers below dataloader.validation_rollout
      #   rollout:
      #     - ${dataloader.validation_rollout}
      #   # video_rollout: ${dataloader.validation_rollout}
      #   every_n_epochs: 20
      #   sample_idx: ${diagnostics.plot.sample_idx}
      #   parameters: ${diagnostics.plot.parameters}
      #   colormaps: ${diagnostics.plot.colormaps}

  debug:
    # this will detect and trace back NaNs / Infs etc. but will slow down training
    anomaly_detection: False


  enable_checkpointing: True
  checkpoint:
    every_n_minutes:
      save_frequency: 120 # Approximate, as this is checked at the end of training steps
      num_models_saved: 3 #-1 # If set to k, saves the 'last' k model weights in the training.
    every_n_epochs:
      save_frequency: 1
      num_models_saved: -1 # If set to -1, all checkpoints are kept ensuring runs can be continued/forked at any point in the training process
    every_n_train_steps:
      save_frequency: null # Does not scale with rollout
      num_models_saved: 0

  enable_progress_bar: True
  print_memory_summary: True

  log:
    interval: 1
    wandb:
      enabled: False
      offline: False
      log_model: False
      project: 'Anemoi'
      entity: rmi
      # logger options (these probably come with some overhead)
      gradients: False
      parameters: False
    tensorboard:
      enabled: False
    mlflow:
      enabled: True
      offline: True
      authentication: True
      log_model: False
      tracking_uri: 'https://mlflow.ecmwf.int'
      experiment_name: ${oc.env:MLFLOW_EXP}
      project_name: 'Anemoi'
      system: True
      terminal: True
      on_resume_create_child: True
      expand_hyperparams: # Which keys in hyperparams to expand
        - config
      http_max_retries: 35
      run_name: ${oc.decode:${oc.env:MLFLOW_RUN}}
