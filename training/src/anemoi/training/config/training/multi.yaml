---
defaults:
  - scalers: multi

# resume or fork a training from a checkpoint last.ckpt or specified in hardware.files.warm_start
run_id: null
fork_run_id: null
transfer_learning: False # activate to perform transfer learning
load_weights_only: False # only load model weights, do not restore optimiser states etc.

# run in deterministic mode ; slows down
deterministic: False

# miscellaneous
precision: 16-mixed

# n-step input
# 1 = single step scheme, X(t-1) used to predict X(t)
# k > 1: n-step scheme, uses [X(t-k), X(t-k+1), ... X(t-1)] to predict X(t)
n_step_input: 2

# n-step output
# 1 = single step output scheme, single model forward predicts X(t)
# l > 1: n-step output scheme, single model forward predicts [X(t), X(t+1), ... X(t+l-1)]
n_step_output: 1

# gradient accumulation across K batches, K >= 1 (if K == 1 then no accumulation)
# the effective batch size becomes num-devices * batch_size * k
accum_grad_batches: 1

num_sanity_val_steps: 6

# clipp gradients, 0 : don't clip, default algorithm: norm, alternative: value
gradient_clip:
  val: 32.
  algorithm: value

# stochastic weight averaging
# https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/
swa:
  enabled: False
  lr: 1.e-4

# =====================================================================
# Optimizer configuration
# =====================================================================
optimizer:
  # ---------------------------------------------------------------
  # Choose optimizer type (_target_ approach)
  # ---------------------------------------------------------------
  # Default optimizer: AdamW
  _target_: torch.optim.AdamW

  # ---------------------------------------------------------------
  # Common optimizer parameters
  # ---------------------------------------------------------------
  # Learning rate is defined elsewhere
  #kwargs:
  betas: [0.9, 0.95]      # β₁, β₂ for Adam-style optimizers

  # ---------------------------------------------------------------
  # Optional: configuration for AdEMAMix (custom optimizer)
  # Uncomment the lines below to enable it
  # ---------------------------------------------------------------
  # _target_: anemoi.training.optimizers.AdEMAMix.AdEMAMix  # Custom optimizer
  # betas: [0.9, 0.95, 0.9999]  # β₁, β₂, β₃
  # alpha: 8.0                   # Mixing factor controlling EMA fusion
  # beta3_warmup: 260000         # Warm-up steps for β₃ (in iterations)
  # alpha_warmup: 260000         # Warm-up steps for α (in iterations)
  # weight_decay: 0.01

  # Optional: configuration for ZeroRedundancyOptimizer
  # _target_: torch.distributed.optim.ZeroRedundancyOptimizer
  # optimizer_class:
  #   _target_: torch.optim.AdamW
  #   _partial_: true
  # betas: [0.9, 0.95]

# select model
model_task: anemoi.training.train.tasks.GraphForecaster

# select strategy
strategy:
  _target_: anemoi.training.distributed.strategy.DDPGroupStrategy
  num_gpus_per_model: ${system.hardware.num_gpus_per_model}
  read_group_size: ${dataloader.read_group_size}

# loss functions

# dynamic rescaling of the loss gradient
# see https://arxiv.org/pdf/2306.06079.pdf, section 4.3.2
# don't enable this by default until it's been tested and proven beneficial
loss_gradient_scaling: False
# length of the "rollout" window (see Keisler's paper)
rollout:
  start: 1
  # increase rollout every n epochs
  epoch_increment: 0
  # maximum rollout to use
  max: 1

# Set max_epochs or max_steps. Training stops at the first limit reached.
max_epochs: null
max_steps: 150000

lr:
  warmup: 1000 # number of warmup iterations
  rate: 0.625e-4 #local_lr
  iterations: ${training.max_steps} # NOTE: When max_epochs < max_steps, scheduler will run for max_steps
  min: 3e-7 #Not scaled by #GPU

# Changes in per-gpu batch_size should come with a rescaling of the local_lr
# in order to keep a constant global_lr
# global_lr = local_lr * num_gpus_per_node * num_nodes / gpus_per_model

submodules_to_freeze: []

# Dataset-specific loss and metrics configuration
training_loss:
  datasets:
    era5:
      # ERA5 uses additional scalers
      _target_: anemoi.training.losses.MSELoss
      scalers: ['pressure_level', 'general_variable', 'nan_mask_weights', 'node_weights', 'time_steps']
      ignore_nans: false
    cerra:
      # CERRA uses additional scalers
      _target_: anemoi.training.losses.MSELoss
      scalers: ['pressure_level', 'general_variable', 'nan_mask_weights', 'node_weights', 'time_steps']
      ignore_nans: false

validation_metrics:
  datasets:
    era5:
      mse:
        _target_: anemoi.training.losses.MSELoss
        scalers: ['node_weights', 'time_steps']
        ignore_nans: true
    cerra:
      mse:
        _target_: anemoi.training.losses.MSELoss
        scalers: ['node_weights', 'time_steps']
        ignore_nans: true

# ERA5-specific variable groups (extends default with more variables)
variable_groups:
  datasets:
    era5:
      default: sfc
      pl:
        param: [q, t, u, v, w] #, z] TODO: # ERA5 has vorticity and divergence
      sfc:
        param: [tp, cp, 10u, 10v, 2d, sp]  # ERA5 surface variables
    cerra:
      default: sfc
      pl:
        param: [q, t, u, v, w] #, z] TODO: # CERRA has vorticity and divergence
      sfc:
        param: [tp, cp, 10u, 10v, 2d]  # CERRA surface variables

# Same metrics as ERA5
metrics:
  datasets:
    era5:
    - z_500
    - z_1000
    - t_500
    - u_850
    - v_850
    cerra:
    - z_500
    - z_1000
    - t_500
    - u_850
