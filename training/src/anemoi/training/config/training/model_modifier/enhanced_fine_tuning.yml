# Enhanced fine-tuning configuration with modular checkpoint loading
# This configuration demonstrates the new modular fine-tuning capabilities

modifiers:
  # First, load pretrained weights from checkpoint
  - _target_: "anemoi.training.train.modify.TransferLearningModelModifier"
    checkpoint_path: null  # Set to path of checkpoint file
    
  # Then, freeze specific layers to prevent over-fitting
  - _target_: "anemoi.training.train.modify.FreezingModelModifier"
    submodules_to_freeze:
      - "encoder"  # Freeze encoder layers
      - "processor.0"  # Freeze first processor layer
      # Add more layers as needed:
      # - "processor.1" 
      # - "decoder.embeddings"

# Example configurations for different fine-tuning strategies:

# 1. For domain adaptation (freeze encoder, fine-tune decoder):
# submodules_to_freeze:
#   - "encoder"
#   - "processor"

# 2. For few-shot learning (freeze most layers, fine-tune only final layers):
# submodules_to_freeze:
#   - "encoder" 
#   - "processor.0"
#   - "processor.1"
#   - "processor.2"

# 3. For parameter-efficient fine-tuning (this would require PEFT integration):
# This configuration can be extended once PEFT is integrated