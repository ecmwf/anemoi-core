activation: GELU
num_channels: 1024
cpu_offload: False
output_mask: null

model:
  _target_: anemoi.models.models.encoder_processor_decoder.AnemoiModelEncProcDec

# LayerKernels are optionally. If not specified, the default are pytorch.nn.LayerNorm and torch.nn.Linear.
layer_kernels:
  processor:
    LayerNorm:
      _target_: torch.nn.LayerNorm
      _partial_: True
      #Any arguments to your chosen function go here
    Linear:
      _target_: torch.nn.Linear
      _partial_: True
      #Any arguments to your chosen function go here
    QueryNorm:
      _target_: anemoi.models.layers.normalization.AutocastLayerNorm
      _partial_: True
      bias: False
      #Any arguments to your chosen function go here
    KeyNorm:
      _target_: anemoi.models.layers.normalization.AutocastLayerNorm
      _partial_: True
      bias: False
      #Any arguments to your chosen function go here
  encoder:
    LayerNorm:
      _target_: torch.nn.LayerNorm
      _partial_: True
      #Any arguments to your chosen function go here
    Linear:
      _target_: torch.nn.Linear
      _partial_: True
      #Any arguments to your chosen function go here
  decoder:
    LayerNorm:
      _target_: torch.nn.LayerNorm
      _partial_: True
      #Any arguments to your chosen function go here e.g.
    Linear:
      _target_: torch.nn.Linear
      _partial_: True
      #Any arguments to your chosen function go here e.g.

processor:
  _target_: anemoi.models.layers.processor.TransformerProcessor
  activation: ${model.activation}
  num_layers: 16
  num_chunks: 2
  mlp_hidden_ratio: 4 # GraphTransformer or Transformer only
  num_heads: 16 # GraphTransformer or Transformer only
  window_size: 512  # 1120
  dropout_p: 0.0 # GraphTransformer
  attention_implementation: flash_attention # flash_attention, scaled_dot_product_attention
  qk_norm: False # Transformer and GraphTransformer only
  softcap: 0.0 # Transformer only
  use_alibi_slopes: False # Transformer only
  cpu_offload: ${model.cpu_offload}


encoder:
  _target_: anemoi.models.layers.mapper.GraphTransformerForwardMapper
  trainable_size: ${model.trainable_parameters.data2hidden}
  sub_graph_edge_attributes: ${model.attributes.edges}
  activation: ${model.activation}
  num_chunks: 1
  mlp_hidden_ratio: 4 # GraphTransformer or Transformer only
  num_heads: 16 # GraphTransformer or Transformer only
  qk_norm: False
  cpu_offload: ${model.cpu_offload}


decoder:
  _target_: anemoi.models.layers.mapper.GraphTransformerBackwardMapper
  trainable_size: ${model.trainable_parameters.hidden2data}
  sub_graph_edge_attributes: ${model.attributes.edges}
  activation: ${model.activation}
  num_chunks: 1
  mlp_hidden_ratio: 4 # GraphTransformer or Transformer only
  num_heads: 16 # GraphTransformer or Transformer only
  initialise_data_extractor_zero: False
  qk_norm: False
  cpu_offload: ${model.cpu_offload}


trainable_parameters:
  data: 8
  hidden: 8
  data2hidden: 8
  hidden2data: 8


attributes:
  edges:
  - edge_length
  - edge_dirs
  nodes: []


# Bounding configuration
bounding: #These are applied in order
  - _target_: anemoi.models.layers.bounding.ReluBounding #[0, infinity)
    variables:
    - tp
    - tcw
    - ssrd
    # - ro
    - q_50
    - q_100
    - q_150
    - q_200
    - q_250
    - q_300
    - q_400
    - q_500
    - q_600
    - q_700
    - q_850
    - q_925
    - q_1000
  - _target_: anemoi.models.layers.bounding.HardtanhBounding #[0, 1)
    variables:
    - tcc
    # - swvl1
    # - swvl2
    min_val: 0
    max_val: 1
  - _target_: anemoi.models.layers.bounding.FractionBounding # fraction of tp
    variables:
    - cp
    - sf
    min_val: 0
    max_val: 1
    total_var: tp
  - _target_: anemoi.models.layers.bounding.FractionBounding # fraction of tp
    variables:
    - lcc
    - mcc
    - hcc
    min_val: 0
    max_val: 1
    total_var: tcc
  # - _target_: anemoi.models.layers.bounding.LeakyHardtanhBounding #[0, 1)
  #   variables:
  #   - swvl1
  #   - swvl2
  #   min_val: 0
  #   max_val: 0.8
