# resume or fork a training from a checkpoint last.ckpt or specified in hardware.files.warm_start
run_id: null
fork_run_id: null
load_weights_only: null # only load model weights, do not restore optimiser states etc.
transfer_learning: null # activate to perform transfer learning

# run in deterministic mode ; slows down
deterministic: False

# miscellaneous
precision: 16-mixed

# multistep input
# 1 = single step scheme, X(t-1) used to predict X(t)
# k > 1: multistep scheme, uses [X(t-k), X(t-k+1), ... X(t-1)] to predict X(t)
# Deepmind use k = 2 in their model
multistep_input: 1  # 2

# gradient accumulation across K batches, K >= 1 (if K == 1 then no accumulation)
# the effective batch size becomes num-devices * batch_size * k
accum_grad_batches: 1

num_sanity_val_steps: 6

# clipp gradients, 0 : don't clip, default algorithm: norm, alternative: value
gradient_clip:
  val: 32.
  algorithm: value

# stochastic weight averaging
# https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/
swa:
  enabled: False
  lr: 1.e-4

# use ZeroRedundancyOptimizer ; saves memory for larger models
zero_optimizer: False

# loss functions

# dynamic rescaling of the loss gradient
# see https://arxiv.org/pdf/2306.06079.pdf, section 4.3.2
# don't enable this by default until it's been tested and proven beneficial

# loss function for the model
training_loss:
  # loss class to initialise
  _target_: anemoi.training.losses.mse.WeightedMSELoss
  # Scalars to include in loss calculation
  # Available scalars include:
  # - 'variable': See `variable_loss_scaling` for more information
  # - 'loss_weights_mask': Giving imputed NaNs a zero weight in the loss function
  # - 'tendency': See `additional_scalars` for more information
  scalars: ['variable', 'tendency', 'variable_pressure_level', 'loss_weights_mask']

  ignore_nans: False

loss_gradient_scaling: False

# Validation metrics calculation,
# This may be a list, in which case all metrics will be calculated
# and logged according to their name.
# These metrics are calculated in the output model space, and thus
# have undergone postprocessing.
validation_metrics:
  # loss class to initialise
  - _target_: anemoi.training.losses.mse.WeightedMSELoss
    # Scalars to include in loss calculation
    # Cannot scale over the variable dimension due to possible remappings.
    # Available scalars include:
    # - 'loss_weights_mask': Giving imputed NaNs a zero weight in the loss function
    # Use the `scale_validation_metrics` section to variable scale.
    scalars: []
    # other kwargs
    ignore_nans: True

# List of validation metrics to keep in normalised space, and scalars to be applied
# Use '*' in reference all metrics, or a list of metric names.
# Unlike above, variable scaling is possible due to these metrics being
# calculated in the same way as the training loss, within the internal model space.
scale_validation_metrics:
  scalars_to_apply: ['variable']
  metrics:
    - 'all'
    # - "*"


# length of the "rollout" window (see Keisler's paper)
rollout:
  start: 1
  # increase rollout every n epochs
  epoch_increment: 0
  # maximum rollout to use
  max: 1

# Set max_epochs or max_steps. Training stops at the first limit reached.
max_epochs: null
max_steps: 320000

lr:
  rate: 0.625e-4 #local_lr
  iterations: 300000
  min: 3e-7 #Not scaled by #GPU
  warmup_t: 1000

# Changes in per-gpu batch_size should come with a rescaling of the local_lr
# in order to keep a constant global_lr
# global_lr = local_lr * num_gpus_per_node * num_nodes / gpus_per_model

# Variable loss scaling
# 'variable' must be included in `scalars` in the losses for this to be applied.
variable_loss_scaling:
  variable_groups:
    default: sfc
    #pl: [q, t, u, v, w, z]
    pl: [q, t, u, v, z]

  default: 1
  q: 1
  t: 1
  u: 1
  v: 1
  #w: 0.1
  z: 1
  sp: 1
  10u: 1
  10v: 1
  2d: 1
  tp: 1
  cp: 1
additional_scalars:
  # pressure level scalar
  - _target_: anemoi.training.train.scaling.ReluVariableLevelScaler
    group: pl
    y_intercept: 0.2
    slope: 0.001
    scale_dim: -1 # dimension on which scaling applied
    name: "variable_pressure_level"
  # stdev tendency scaler
  # scale the prognostic losses by the stdev of the variable tendencies (e.g. the 6-hourly differences of the data)
  # useful if including slow vs fast evolving variables in the training (e.g. Land/Ocean vs Atmosphere)
  # if using this option 'variable_loss_scalings' should all be set close to 1.0 for prognostic variables
  - _target_: anemoi.training.train.scaling.StdevTendencyScaler
    scale_dim: -1 # dimension on which scaling applied
    name: "tendency"
  # var tendency scaler (this should be default!?)
  # - _target_: anemoi.training.train.scaling.VarTendencyScaler
  #   scale_dim: -1 # dimension on which scaling applied
  #   name: "tendency"

metrics:
- z_500
- t_850
- u_850
- v_850

node_loss_weights:
  _target_: anemoi.training.losses.nodeweights.GraphNodeAttribute
  target_nodes: ${graph.data}
  node_attribute: area_weight
